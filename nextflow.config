/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    freyr Nextflow config file
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Default config options for all compute environments
----------------------------------------------------------------------------------------
*/

// Global default params, used in configs
params {

    help                        = null

    ///// Input options
    samplesheet                 = null                              // location of samplesheet .csv; path   
    primer_params               = null                              // location of primer parameters .csv; path
    train_idtaxa                = null                              // train IDTAXA model on ref_fasta database; boolean

    extension                   = null                              // extension of the .fastq read files, immediately after the digit indicating forward/reverse identity; string
    seq_type                    = "illumina"                        // data input is Illumina reads; string (illumina, nanopore, pacbio)
    paired                      = true                              // Illumina data input is paired-end; boolean
    miseq_internal              = false                             // data generated by an AgVic MiSeq machine; boolean
    miseq_dir                   = null                              // directory containing MiSeq data, required when using '--miseq_internal'; path

    primer_error_rate           = 1                                 // error rate allowed when matching primers with cutadapt ('cutadapt -e'); number
    primer_n_trim               = false                             // use '--match-read-wildcards' with cutadapt in PRIMER_TRIM to recognise N bases in reads

    /// TODO: replace this with equivalent input-file option
    high_sensitivity            = true                              // enable dada2 high-sensitivity (pseudo-pooling) mode; boolean
    dada_band_size              = 16                                // set BAND_SIZE parameter for dada2::dada function in 'denoise.R'; integer
    dada_homopolymer            = null                              // set HOMOPOLYMER_GAP_PENALTY parameter for dada2::dada function in 'denoise.R'; integer < 0

    chimera_sample_frac         = 0.9                               // minimum fraction of samples in which a sequence must be flagged as a chimera to be classified a chimera.; see dada2::isBimeraDenovoTable(minSampleFraction); float (0-1)

    chunk_taxassign             = 100                               // number of sequences to be taxonomically assigned per process; integer > 0

    accumulation_curve          = true                              // generate accumulation curves per sample for each primer pair; boolean

    merge_clusters              = null                              // generate separate output files for filtered ASVs merged by cluster

    ///// primer_params override options
    pp_primers                  = null
    pp_locus                    = null
    pp_for_primer_seq           = null
    pp_rev_primer_seq           = null
    pp_max_primer_mismatch      = null
    pp_read_min_length          = null 
    pp_read_max_length          = null 
    pp_read_max_ee              = null 
    pp_read_trunc_length        = null 
    pp_read_trim_left           = null 
    pp_read_trim_right          = null 
    pp_asv_min_length           = null 
    pp_asv_max_length           = null 
    pp_concat_unmerged          = null 
    pp_genetic_code             = null 
    pp_coding                   = null 
    pp_phmm                     = null 
    pp_idtaxa_db                = null 
    pp_ref_fasta                = null 
    pp_idtaxa_confidence        = null 
    pp_run_blast                = null 
    pp_blast_min_identity       = null 
    pp_blast_min_coverage       = null 
    pp_cluster_threshold        = null
    pp_target_kingdom           = null 
    pp_target_phylum            = null 
    pp_target_class             = null 
    pp_target_order             = null 
    pp_target_family            = null 
    pp_target_genus             = null 
    pp_target_species           = null 
    pp_min_sample_reads         = null 
    pp_min_taxa_reads           = null 
    pp_min_taxa_ra              = null 
    
    ///// debugging options
    rdata                       = false                     // save all data/objects from process-level R sessions as .RData files in work dir; boolean
    subsample                   = null                      // reduce input samples (in ch_sample_locus_reads) to given number; integer
    downsample_reads            = null                      // reduce input reads per sample to at most this many; integer

    slurm_account               = "default"                 // account to use on SLURM; default is for BASC only

    ///// Max resource options
    // Defaults only, expecting to be overwritten
    max_memory                 = '128.GB'
    max_cpus                   = 16
    max_time                   = '240.h'

}

validation {

    failUnrecognisedParams      = true                      // run will fail if unrecognised parameters are given, not just give a warning

}

profiles {
    debug {
        params.rdata           = true
    }
    apptainer {
        apptainer.enabled      = true
        apptainer.autoMounts   = true
        charliecloud.enabled   = false
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = false        
        shifter.enabled        = false
        singularity.enabled    = false
    }
    //// this profile is not enabled because charliecloud does not seem to work 
    // charliecloud {
    //     apptainer.enabled      = false
    //     charliecloud.enabled   = true
    //     conda.enabled          = false
    //     docker.enabled         = false
    //     podman.enabled         = false        
    //     shifter.enabled        = false
    //     singularity.enabled    = false
    // }
    docker {
        apptainer.enabled      = false
        charliecloud.enabled   = false
        conda.enabled          = false
        docker.enabled         = true
        docker.runOptions      = '-u $(id -u):$(id -g)'
        podman.enabled         = false        
        shifter.enabled        = false
        singularity.enabled    = false
    }
    podman {
        apptainer.enabled      = false
        charliecloud.enabled   = false
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = true        
        shifter.enabled        = false
        singularity.enabled    = false
    }
    shifter {
        apptainer.enabled      = false
        charliecloud.enabled   = false
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = false        
        shifter.enabled        = true
        singularity.enabled    = false
    }
    singularity {
        apptainer.enabled      = false
        charliecloud.enabled   = false
        conda.enabled          = false
        docker.enabled         = false
        podman.enabled         = false        
        shifter.enabled        = false
        singularity.enabled    = true
        singularity.autoMounts = true
    }
    basc_slurm {
        process.executor            = 'slurm'
        process.queue               = 'batch,shortrun'
        params.slurm_account        = "ngdsi"
        process.clusterOptions      = "--account $params.slurm_account"
        params.max_memory           = '512.GB'
        params.max_time             = '168.h'
        params.max_cpus             = 48
        executor.queueSize          = 200
        executor.pollInterval       = '10 sec'
        executor.submitRateLimit    = '5 sec'
        process.module              = 'shifter' // this runs 'module load shifter' at the start of each process job
        shifter.enabled             = true
    }
    test { /// this profile should always be specified last to force the minimal resource requirements
        params.samplesheet          = 'test_data/dual/samplesheet_read_dir.csv'
        params.primer_params        = "test_data/dual/primer_params.csv"
        params.miseq_dir            = "test_data/dual"
        params.max_memory           = '2.GB'
        params.max_time             = '10.m'
        params.max_cpus             = 1
        params.miseq_internal       = true
        params.seq_type             = "illumina"
        params.paired               = true
    }
    quick {
        params.max_time             = '10.m'
    }
    error1_backoff {
        process.errorStrategy       = {
                                        if ( task.exitStatus in ((130..145) + 104) ) {
                                            'retry'
                                        } else if ( task.exitStatus == 1 ) {
                                            sleep(Math.pow(2, task.attempt) * 2 as long); return 'retry'
                                        } else {
                                            'finish'
                                        }
                                    }
    }
}

process {

    // error handling
    errorStrategy = { task.exitStatus in ((130..145) + 104) ? 'retry' : 'finish' }
    maxRetries    = 3
    maxErrors     = '-1'
    withLabel: error_retry {
        errorStrategy = 'retry'
        maxRetries    = 3
    }

    withName: TAX_IDTAXA {
        cpus    = { check_max( 1                    , 'cpus'    ) }
        memory  = { check_max( 16.GB * task.attempt , 'memory'  ) }
        time    = { check_max( 6.h * task.attempt   , 'time'    ) }
    }

    // resources
    withLabel: very_small {
        cpus    = { check_max( 1                  , 'cpus'    ) }
        memory  = { check_max( 2.GB * task.attempt, 'memory'  ) }
        time    = { check_max( 10.m * task.attempt, 'time'    ) }
    }
    withLabel: small {
        cpus    = { check_max( 1                  , 'cpus'    ) }
        memory  = { check_max( 4.GB * task.attempt, 'memory'  ) }
        time    = { check_max( 30.m * task.attempt, 'time'    ) }
    }
    withLabel: medium {
        cpus    = { check_max( 1                  , 'cpus'    ) }
        memory  = { check_max( 8.GB * task.attempt, 'memory'  ) }
        time    = { check_max( 2.h * task.attempt, 'time'     ) }
    }
    withLabel: high {
        cpus    = { check_max( 4 * task.attempt   , 'cpus'    ) }
        memory  = { check_max( 16.GB * task.attempt, 'memory'  ) }
        time    = { check_max( 2.h *  task.attempt, 'time'    ) }
    }
    withLabel: very_high {
        cpus    = { check_max( 16 * task.attempt  , 'cpus'    ) }
        memory  = { check_max( 64.GB * task.attempt, 'memory'  ) }
        time    = { check_max( 2.h *  task.attempt, 'time'    ) }
    }
    withLabel: long {
        time    = { check_max( 4.h * task.attempt, 'time'    ) }
    }
    withLabel: phyloseq {
        cpus    = { check_max( 1  , 'cpus'    ) }
        memory  = { check_max( 16.GB * task.attempt, 'memory'  ) }
        time    = { check_max( 1.h * task.attempt, 'time'    ) }
    }
}



plugins {
    id 'nf-schema@2.0.0'                            // create schema to validate sample sheets and pipeline parameters
}

report {
    enabled             = true
    overwrite           = true
    file                = "output/run_info/report.html"
}

trace {
    enabled             = true
    overwrite           = true
    file                = "output/run_info/trace.tsv"
}

dag {
    enabled             = true
    overwrite           = true
    file                = "output/run_info/dag.html"
    verbose             = true
}

timeline {
    enabled             = true
    overwrite           = true
    file                = "output/run_info/timeline.html"
}

// Function to ensure that resource requirements don't go beyond a maximum limit
// from: https://github.com/nf-core/tools/blob/99961bedab1518f592668727a4d692c4ddf3c336/nf_core/pipeline-template/nextflow.config#L206-L237
def check_max(obj, type) {
    if (type == 'memory') {
        try {
            if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                return params.max_memory as nextflow.util.MemoryUnit
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'time') {
        try {
            if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                return params.max_time as nextflow.util.Duration
            else
                return obj
        } catch (all) {
            println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
            return obj
        }
    } else if (type == 'cpus') {
        try {
            return Math.min( obj, params.max_cpus as int )
        } catch (all) {
            println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
            return obj
        }
    }
}
