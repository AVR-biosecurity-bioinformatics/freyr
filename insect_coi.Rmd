---
title: "iMapPESTS local metabarcoding workflow"
author: "A.M. Piper"
date: "`r Sys.Date()`"
output:
  
  html_document:
    highlighter: null
    theme: "flatly"
    code_download: true
    code_folding: show
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    df_print: paged    
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knitr global setup - change eval to true to run code
library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE, fig.show = "hold", fig.keep = "all")
opts_chunk$set(dev = 'png')
```

# Demultiplex sequencing reads
For this workflow to run, we need to first demultiplex the miseq run again as the miseq does not put indexes in fasta headers by default, and also obtain some necessary files from the sequencing folder. The below code is written for the Agriculture Victoria BASC server, and the locations will be different if you are using a different HPC cluster.

The output directory should be unique for each sequencing run, named as the flowcell id, within a directory called data

For example:

    root/
      ├── data/
         ├── CJL7D/
         
```{bash demultiplex 1 mismatch}
#load module
module load bcl2fastq2/2.20.0-foss-2018b

#raise amount of available file handles
ulimit -n 4000

###Run1

#Set up input and outputs
inputdir=/group/sequencing/210219_M03633_0489_000000000-JDYG3 #CHANGE TO YOUR SEQ RUN
outputdir=/group/pathogens/IAWS/Projects/Metabarcoding/dros_surveillance/data/JDYG3 #CHANGE TO YOUR DATA FOLDER RUN
samplesheet=/group/pathogens/IAWS/Projects/Metabarcoding/dros_surveillance/SampleSheet_JDYG3.csv #CHANGE TO YOUR SAMPLESHEET

# convert samplesheet to unix format
dos2unix $samplesheet

#Demultiplex
bcl2fastq -p 12 --runfolder-dir $inputdir \
--output-dir $outputdir \
--sample-sheet $samplesheet \
--no-lane-splitting --barcode-mismatches 1

# Copy other necessary files and move fastqs
cd $outputdir
cp -r $inputdir/InterOp $outputdir
cp $inputdir/RunInfo.xml $outputdir
cp $inputdir/runParameters.xml $outputdir
cp $samplesheet $outputdir
mv **/*.fastq.gz $outputdir

# Append fcid to start of sample names if missing
fcid=$(echo $inputdir | sed 's/^.*-//')
for i in *.fastq.gz; do
  if ! [[ $i == $fcid* ]]; then
  new=$(echo ${fcid} ${i}) #append together
  new=$(echo ${new// /_}) #remove any whitespace
  mv -v "$i" "$new"
  fi
done

```

# Optional: Run R on BASC
You may wish to run this workflow through the BASC command line in order to take advantage of more processing power. To do this, you can start a new SLURM interactive session. Press the CODE button to the lower right to display the code for this optional step.

```{bash, class.source = 'fold-hide'}
cd /group/pathogens/IAWS/Projects/Metabarcoding/dros_surveillance
# Create new interactive SLURM session
sinteractive --ntasks=1 --cpus-per-task=10 --mem-per-cpu=10GB --time=72:00:00

module load R/4.2.0-foss-2021b
module load pkgconfig/1.5.1-GCCcore-9.3.0-Python-3.8.2
module load GDAL/3.3.0-foss-2021a
module load BLAST+/2.11.0-gompi-2020a
module load Pandoc/2.5

# Load R
R

# Run quit() to quit R once you are finished
```


# Install and load R packages and setup directories {.tabset}

The seqateurs R package also provides wrappers around other software packages for QC. For convenience we will download and install these software in a new folder called "bin"

```{r Manual install} 
#Set required packages
.cran_packages <- c(
  "devtools",
  "ggplot2",
  "gridExtra",
  "data.table",
  "tidyverse", 
  "stringdist",
  "patchwork",
  "vegan",
  "seqinr",
  "patchwork",
  "stringi",
  "phangorn",
  "magrittr",
  "galah"
  )

.bioc_packages <- c(
  "phyloseq",
  "DECIPHER",
  "Biostrings",
  "ShortRead",
  "ggtree",
  "savR",
  "dada2",
  "ngsReports"
  )

.inst <- .cran_packages %in% installed.packages()
if(any(!.inst)) {
   install.packages(.cran_packages[!.inst])
}
.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)) {
  if (!requireNamespace("BiocManager", quietly = TRUE)){
    install.packages("BiocManager")
  }
  BiocManager::install(.bioc_packages[!.inst], ask = F)
}

#Load all published packages
sapply(c(.cran_packages,.bioc_packages), require, character.only = TRUE)

# Install and load github packages
devtools::install_github("alexpiper/seqateurs", dependencies = TRUE)
library(seqateurs)

devtools::install_github("alexpiper/taxreturn", dependencies = TRUE)
library(taxreturn)

devtools::install_github("mikemc/speedyseq", dependencies = TRUE)
library(speedyseq)


library(targets)
library(tarchetypes)
source("R/functions.R")


source("R/dependencies.R")
source("R/functions.R")
source("R/themes.R")
```


# Create sample sheet 

The directory structure should now look something like this:

    root/
    ├── data/
    │   ├── CJL7D/
    │   │  ├── R1.fastq.gz
    │   │  ├── R2.fastq.gz
    │   │  ├── runInfo.xml
    │   │  ├── runParameters.xml
    │   │  ├── SampleSheet.csv
    │   │  └── InterOp/
    │   └── fcid2/
    ├── sample_data/
    ├── reference
    ├── bin
    ├── output/
    └── doc/

The reference and bin folders can be copied from previous runs. 

In order to track samples and relevant QC statistics throughout the metabarcoding pipeline, we will first create a new samplesheet from our input samplesheets. This function requires both the SampleSheet.csv used for the sequencing run, and the runParameters.xml, both of which should have been automatically obtained from the demultiplexed sequencing run folder in the bash step above

```{r create samplesheet}
runs <- dir("data/") #Find all directories within data
SampleSheet <- list.files(paste0("data/", runs), pattern= "SampleSheet", full.names = TRUE)
runParameters <- list.files(paste0("data/", runs), pattern= "[Rr]unParameters.xml", full.names = TRUE)

# Create samplesheet containing samples and run parameters for all runs
samdf <- create_samplesheet(SampleSheet = SampleSheet, runParameters = runParameters, template = "V4") %>%
  distinct()

# Filter out the run where the reverse reads failed
samdf <- samdf %>%
  dplyr::filter(!fcid == "KJ2DL")

# Check if sampleids contain fcid, if not; attatch
samdf <- samdf %>%
  mutate(sample_id = case_when(
    !str_detect(sample_id, fcid) ~ paste0(fcid,"_",sample_id),
    TRUE ~ sample_id
  ))

# Check if samples match samplesheet
fastqFs <- purrr::map(list.dirs("data", recursive=FALSE),
                      list.files, pattern="_R1_", full.names = TRUE) %>%
  unlist() %>%
  str_remove(pattern = "^(.*)\\/") %>%
  str_remove(pattern = "(?:.(?!_S))+$")
fastqFs <- fastqFs[!str_detect(fastqFs, "Undetermined")]

#Check missing in samplesheet
if (length(setdiff(fastqFs, samdf$sample_id)) > 0) {warning("The fastq file/s: ", setdiff(fastqFs, samdf$sample_id), " are not in the sample sheet") }

#Check missing fastqs
if (length(setdiff(samdf$sample_id, fastqFs)) > 0) {
  warning(paste0("The fastq file: ",
                 setdiff(samdf$sample_id, fastqFs),
                 " is missing, dropping from samplesheet \n")) 
  samdf <- samdf %>%
    filter(!sample_id %in% setdiff(samdf$sample_id, fastqFs))
}

write_csv(samdf, "sample_data/Sample_info.csv")
```

# Add PCR primers to sample sheet {.tabset}

This can either be done manually by editing the sample_data/Sample_info.csv file, or it can be done in R as below.

If a single primer set was used across all samples, these can simply be added using a mutate call (**tab 1**).

If different primers were used for different samples, these can be set using pattern matching on the sample names with case_when (**tab 2**).

If multiple primer sets are used per sample, before indexing, the pipeline will conduct an extra round of demultiplexing to split each sample by primer. This option can be set by splitting each primer set with a semicolon (**tab 3**) 

## Single primer set

```{R}
# Add primers to sample sheet
samdf <- samdf %>%
  mutate(pcr_primers = "fwhF2-fwhR2n",
  for_primer_seq = "GGDACWGGWTGAACWGTWTAYCCHCC",
  rev_primer_seq = "GTRATWGCHCCDGCTARWACWGG"
  )

write_csv(samdf, "sample_data/Sample_info.csv")
```

## Different primer sets per sample

```{R}
# Add PCR primers to sample sheet
samdf <- samdf %>%
  mutate(pcr_primers = case_when(
    str_detect(sample_name, "primer1") ~ "fwhF2-fwhR2n",
    str_detect(sample_name, "primer2") ~ "EIF3LminiF4-EIF3lminiR4"
  ),
  for_primer_seq = case_when(
    str_detect(sample_name, "primer1") ~ "GGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(sample_name, "primer2") ~ "GATGCGYCGTTATGCYGATGC"
  ),
  rev_primer_seq = case_when(
    str_detect(sample_name, "primer1") ~ "GTRATWGCHCCDGCTARWACWGG",
    str_detect(sample_name, "primer2") ~ "TTRAAYACTTCYARATCRCC"
  ))


write_csv(samdf, "sample_data/Sample_info.csv")
```

## Multiple primer sets per sample

```{R}
# Add PCR primers to sample sheet
samdf <- samdf %>%
  mutate(
    pcr_primers = "fwhF2-fwhR2nDac;EIF3LminiF4-EIF3lminiR4",
    for_primer_seq = "GGDACWGGWTGAACWGTWTAYCCHCC;GATGCGYCGTTATGCYGATGC",
    rev_primer_seq = "GTRATWGCHCCIGCTAADACHGG;TTRAAYACTTCYARATCRCC"
    )

write_csv(samdf, "sample_data/Sample_info.csv")
```


# Create parameters file {.tabset}

The parameters file table the respective target gene, reference databases, and filtering parameters for each primer set used to amplify the samples. As the pcr_primers column is used in the pipeline to match the parameters to the respective sample, it is critical that the primer names match those set in the previous step.

If a single primer set was used across all samples, these can simply be added to the table as below (**tab 1**).

If different primers were used for different samples, these can be set as different rows in the table using c() when creatign the tibble (**tab 2**).

If multiple primer reference databases are to be used for each sample, these can be set by splitting each a semicolon. The taxonomic assignment will be conducted sequentially through the databases from left to right, with the second reference database only being used for those ASVs that couldnt be assigned to species level using the first databases (**tab 3**) 

The genetic code parameter is used to check coding markers for stop codons and frameshifts that commonly indicate pseudogenes. For COI amplicons from insects, the invertebrate mitochondrial code is "SGC4", while the standard genetic code is "SGC0". If you are targetting other groups of organisms, all alternative genetic codes can be found by running Biostrings::GENETIC_CODE_TABLE in R.


## Single primer set

```{R}
# Params to add in step_add_parameters
params <- tibble(
  pcr_primers = "fwhF2-fwhR2n",
  target_gene="COI",
  phmm = "reference/folmer_fullength_model.rds",
  ref_db = "reference/idtaxa_bftrimmed.rds",
  blast_db = "reference/insecta_hierarchial_bftrimmed.fa.gz",
  exp_length = 205,
  genetic_code = "SGC4",
  coding = TRUE
)

write_csv(params, "sample_data/loci_params.csv")
```

## Different or multiple primer sets per sample

```{R}
# Params to add in step_add_parameters
params <- tibble(
  pcr_primers = c("fwhF2-fwhR2nDac", "EIF3LminiF4-EIF3lminiR4"),
  target_gene=c("COI", "EIF3L"),
  phmm = c("reference/phmm/Bactrocera_COI.rds", "reference/phmm/Bactrocera_EIF3L.rds"),
  ref_db = c("reference/COI_idtaxa.rds","reference/EIF3L_idtaxa.rds"),
  blast_db = c("reference/COI_hierarchial.fa.gz", "reference/EIF3L_hierarchial.fa.gz"),
  exp_length = c(205, 217),
  genetic_code = c("SGC4", "SGC0"),
  coding = c(TRUE, TRUE)
)

write_csv(params, "sample_data/loci_params.csv")
```

## Multiple reference databases

```{r}
params <- tibble(
  pcr_primers = c("fwhF2-fwhR2nDac", "EIF3LminiF4-EIF3lminiR4"),
  target_gene=c("COI", "EIF3L"),
  phmm = c("reference/phmm/Bactrocera_COI.rds", "reference/phmm/Bactrocera_EIF3L.rds"),
  ref_db = c("reference/COI_internal_idtaxa.rds;reference/COI_idtaxa.rds","reference/EIF3L_internal_idtaxa.rds;reference/EIF3L_idtaxa.rds"),
  blast_db = c("reference/COI_internal.fa.gz;reference/COI_hierarchial.fa.gz", "reference/EIF3L_internal.fa.gz;reference/EIF3L_hierarchial.fa.gz"),
  exp_length = c(205, 217),
  genetic_code = c("SGC4", "SGC0"),
  coding = c(TRUE, TRUE)
)

write_csv(params, "sample_data/loci_params.csv")
```

# Run pipeline {-}

Now that the sample data sheet and parameters are defined, the pipeline can now be run. This will q
```{r}
tar_make()
```

