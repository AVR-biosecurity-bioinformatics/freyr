---
title: "PipeRline"
subtitle: "General example"
author: "A.M. Piper"
date: "`r Sys.Date()`"
output:
  
  html_document:
    highlighter: null
    theme: "flatly"
    code_download: true
    code_folding: show
    toc: true
    toc_float: 
      collapsed: false
      smooth_scroll: true
    df_print: paged    
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# Knitr global setup - change eval to true to run code
library(knitr)
library(targets)
library(tarchetypes)

knitr::opts_chunk$set(echo = TRUE, eval=FALSE, message=FALSE,error=FALSE, fig.show = "hold", fig.keep = "all")
opts_chunk$set(dev = 'png')
```

# Introduction

This metabarcoding pipeline is based around the targets package, which is a Make-like pipeline tool for R. The benefit of this is that all the code is automatically run, and the pipeline skips costly runtime for tasks that are already up to date.

This page lists the general workflow and all the options that can be adapted for your dataset.

# Clone the pipeRline github repository

The first step is to clone this github repository, which contains the required code and directory structure to run the pipeline. If you are running the pipeline in Rstudio, it is best to create a new project from the github repository. This can be done by going **file > new project > version control > git** then adding https://github.com/alexpiper/piperline.git then change the name of the project to whatever you are working with. 

Alternatively, the repository can be cloned using the command line, change the 'folder-name' to the desired name
```{bash}
# Change into the main directory you wish to make the project in
cd metabarcoding

# Clone the repository
git clone https://github.com/alexpiper/piperline.git folder-name
```

# Prior to analysis
The PipeRline workflow assumes that your sequencing data meets certain criteria:

1. Samples have been demultiplexed, i.e. split into individual per-sample fastq files. If you want the pipeline to calculate the index-switching rate, the fastq files need to be re-demultiplexed as the miseq does not put indexes in fasta headers by default
2. For paired-end sequencing data, the forward and reverse reads are in separate files (ie. not interleaved) with reads arranged in matched order.

The output directory should be unique for each sequencing run, named as the flowcell id, within a directory called data

For example:

    root/
      ├── data/
         ├── CJL7D/
         
## Demultiplex sequencing reads
For this workflow to run, we need to first demultiplex the miseq run again as the miseq does not put indexes in fasta headers by default, and also obtain some necessary files from the sequencing folder. The below code is written for the Agriculture Victoria BASC server, and the locations will be different if you are using a different HPC cluster.

```{bash demultiplex 1 mismatch}
#load module
module load bcl2fastq2/2.20.0-foss-2018b

#raise amount of available file handles
ulimit -n 4000

###Run1

#Set up input and outputs
inputdir=/group/sequencing/210219_M03633_0489_000000000-JDYG3 #CHANGE TO YOUR SEQ RUN
outputdir=/group/pathogens/IAWS/Projects/Metabarcoding/dros_surveillance/data/JDYG3 #CHANGE TO YOUR DATA FOLDER RUN
samplesheet=/group/pathogens/IAWS/Projects/Metabarcoding/dros_surveillance/SampleSheet_JDYG3.csv #CHANGE TO YOUR SAMPLESHEET

# convert samplesheet to unix format
dos2unix $samplesheet

#Demultiplex
bcl2fastq -p 12 --runfolder-dir $inputdir \
--output-dir $outputdir \
--sample-sheet $samplesheet \
--no-lane-splitting --barcode-mismatches 1

# Copy other necessary files and move fastqs
cd $outputdir
cp -r $inputdir/InterOp $outputdir
cp $inputdir/RunInfo.xml $outputdir
cp $inputdir/runParameters.xml $outputdir
cp $samplesheet $outputdir
mv **/*.fastq.gz $outputdir

# Append fcid to start of sample names if missing
fcid=$(echo $inputdir | sed 's/^.*-//')
for i in *.fastq.gz; do
  if ! [[ $i == $fcid* ]]; then
  new=$(echo ${fcid} ${i}) #append together
  new=$(echo ${new// /_}) #remove any white space
  mv -v "$i" "$new"
  fi
done

```


## Download example sequencing reads

If you do not yet have any data, some test sequencing reads have been hosted on Zenodo for testing purposes. The below code will download these and put them inside the data folder.

```{r}
# Create directory for data
if(!dir.exists("data/K77JP")) {dir.create("data/K77JP", recursive = TRUE)}
if(!dir.exists("data/K77JP/InterOp")) {dir.create("data/K77JP/InterOp", recursive = TRUE)}

# Download files from zenodo
download_zenodo(
doi = "10.5281/zenodo.7112162",
path = "data/K77JP"
)

# Move the interop files to the interop folder
fs::dir_ls(path="data/K77JP", glob="*.bin") %>%
  purrr::map(function(x){
    fs::file_copy(path = x, new_path = x %>% str_replace("data/K77JP", "data/K77JP/InterOp"))
    file.remove(x)
  })
```

# Install and load R packages and setup directories {.tabset}
```{r Manual install} 
# Load the targets and renv packages
library(targets)
library(tarchetypes)

# Load all packages using renv::restore() and source package list
renv::restore()
source("_targets_packages.R")

# Source ancillary functions
source("R/functions.R")
source("R/themes.R")
```

# Download reference databases

The pipeRline workflow uses the IDTAXA and BLAST software to assign taxonomy to the sequence reads. These require a pre-trained IDTAXA model, and a fasta file with heirarchial taxonomy like below:

```{code}
>Accession|Root;Kingdom;Phylum;Class;Order;Family;Genus;Species
ACCTAGAAAGTCGTAGATCGAAGTTGAAGCATCGCCCGATGATCGTCTGAAGCTGTAGCATGAGTCGATTTTCACATTCAGGGATACCATAGGATAC
>Root;Kingdom;Phylum;Class;Order;Family;Genus;Species
CGCTAGAAAGTCGTAGAAGGCTCGGAGGTTTGAAGCATCGCCCGATGGGATCTCGTTGCTGTAGCATGAGTACGGACATTCAGGGATCATAGGATAC
```

Here is an example from an existing database:
```{code}
ABC001|2029192;Metazoa;Arthropoda;Insecta;Diptera;Tephritidae;Bactrocera;Bactrocera abscondita
ACTATCATCTATTATTGCTCACGGAGGAGCATCAGTTGATCTAGCTATTTTTTCACTCCACTTAGCGGGTATTTCCTCAATTCTAGGAGCTGTCAATTTCATTAC
>ALB002|164677;Metazoa;Arthropoda;Insecta;Diptera;Tephritidae;Bactrocera;Bactrocera albistrigata
CCTATCATCTGTNATTGCTCACGGAGGAGCTTCAGTTGATCTAGCTATTTTCTCACTTCACCTAGCNGGTATTTCCTCAATTTTAGGGGCAGTTAATTTTATTAC
```

# Create sample tracking sheet

In order to track samples and relevant QC statistics throughout the metabarcoding pipeline, we will first create a new samplesheet from our input samplesheets. This function requires both the SampleSheet.csv used for the sequencing run, and the runParameters.xml, both of which should have been automatically obtained from the demultiplexed sequencing run folder in the bash step above

```{r create samplesheet}
runs <- dir("data/") #Find all directories within data
SampleSheet <- list.files(paste0("data/", runs), pattern= "SampleSheet", full.names = TRUE)
runParameters <- list.files(paste0("data/", runs), pattern= "[Rr]unParameters.xml", full.names = TRUE)

# Create samplesheet containing samples and run parameters for all runs
samdf <- create_samplesheet(SampleSheet = SampleSheet, runParameters = runParameters, template = "V4") %>%
  distinct()

# Check that sample_ids contain fcid, if not; attatch
samdf <- samdf %>%
  mutate(sample_id = case_when(
    !str_detect(sample_id, fcid) ~ paste0(fcid,"_",sample_id),
    TRUE ~ sample_id
  ))

# Check that samples match samplesheet
fastqFs <- purrr::map(list.dirs("data", recursive=FALSE),
                      list.files, pattern="_R1_", full.names = TRUE) %>%
  unlist() %>%
  str_remove(pattern = "^(.*)\\/") %>%
  str_remove(pattern = "(?:.(?!_S))+$")

# Filter undetermined reads from sample sheet
fastqFs <- fastqFs[!str_detect(fastqFs, "Undetermined")]

# Check for fastq files that are missing from samplesheet
if (length(setdiff(fastqFs, samdf$sample_id)) > 0) {warning("The fastq file/s: ", setdiff(fastqFs, samdf$sample_id), " are not in the sample sheet") }

# Check for sample_ids that dont have a corresponding fastq file
if (length(setdiff(samdf$sample_id, fastqFs)) > 0) {
  warning(paste0("The fastq file: ",
                 setdiff(samdf$sample_id, fastqFs),
                 " is missing, dropping from samplesheet \n")) 
  samdf <- samdf %>%
    filter(!sample_id %in% setdiff(samdf$sample_id, fastqFs))
}

# Write out sample tracking sheet
write_csv(samdf, "sample_data/Sample_info.csv")
```

# Add PCR primers to sample sheet {.tabset}

This can either be done manually by editing the sample_data/Sample_info.csv file, or it can be done in R as below.

If a single primer set was used across all samples, these can simply be added using a mutate call (**tab 1**).

If different primers were used for different samples, these can be set using pattern matching on the sample names with case_when (**tab 2**).

If multiple primer sets are used per sample, before indexing, the pipeline will conduct an extra round of demultiplexing to split each sample by primer. This option can be set by splitting each primer set with a semicolon (**tab 3**) 

## Single primer set

```{R}
# Add primers to sample sheet
samdf <- samdf %>%
  mutate(pcr_primers = "fwhF2-fwhR2n",
  for_primer_seq = "GGDACWGGWTGAACWGTWTAYCCHCC",
  rev_primer_seq = "GTRATWGCHCCDGCTARWACWGG"
  )

write_csv(samdf, "sample_data/Sample_info.csv")
```

## Different primer sets per sample

```{R}
# Add PCR primers to sample sheet
samdf <- samdf %>%
  mutate(pcr_primers = case_when(
    str_detect(sample_name, "primer1") ~ "fwhF2-fwhR2n",
    str_detect(sample_name, "primer2") ~ "EIF3LminiF4-EIF3lminiR4"
  ),
  for_primer_seq = case_when(
    str_detect(sample_name, "primer1") ~ "GGDACWGGWTGAACWGTWTAYCCHCC",
    str_detect(sample_name, "primer2") ~ "GATGCGYCGTTATGCYGATGC"
  ),
  rev_primer_seq = case_when(
    str_detect(sample_name, "primer1") ~ "GTRATWGCHCCDGCTARWACWGG",
    str_detect(sample_name, "primer2") ~ "TTRAAYACTTCYARATCRCC"
  ))


write_csv(samdf, "sample_data/Sample_info.csv")
```

## Multiple primer sets per sample

```{R}
# Add PCR primers to sample sheet
samdf <- samdf %>%
  mutate(
    pcr_primers = "fwhF2-fwhR2nDac;EIF3LminiF4-EIF3lminiR4",
    for_primer_seq = "GGDACWGGWTGAACWGTWTAYCCHCC;GATGCGYCGTTATGCYGATGC",
    rev_primer_seq = "GTRATWGCHCCIGCTAADACHGG;TTRAAYACTTCYARATCRCC"
    )

write_csv(samdf, "sample_data/Sample_info.csv")
```


# Create parameters file {.tabset}

The parameters file table the respective target gene, reference databases, and filtering parameters for each primer set used to amplify the samples. As the pcr_primers column is used in the pipeline to match the parameters to the respective sample, it is critical that the primer names match those set in the previous step.

If a single primer set was used across all samples, these can simply be added to the table as below (**tab 1**).

If different primers were used for different samples, these can be set as different rows in the table using c() when creatign the tibble (**tab 2**).

If multiple primer reference databases are to be used for each sample, these can be set by splitting each a semicolon. The taxonomic assignment will be conducted sequentially through the databases from left to right, with the second reference database only being used for those ASVs that couldnt be assigned to species level using the first databases (**tab 3**) 

The genetic code parameter is used to check coding markers for stop codons and frameshifts that commonly indicate pseudogenes. For COI amplicons from insects, the invertebrate mitochondrial code is "SGC4", while the standard genetic code is "SGC0". If you are targetting other groups of organisms, all alternative genetic codes can be found by running Biostrings::GENETIC_CODE_TABLE in R.

## Single primer set

```{R}
# Params to add in step_add_parameters
params <- tibble(
  # Run parameters
  pcr_primers = "fwhF2-fwhR2n", 
  target_gene="COI",
  phmm = "reference/folmer_fullength_model.rds",
  ref_db = "reference/idtaxa_bftrimmed.rds",
  blast_db = "reference/insecta_hierarchial_bftrimmed.fa.gz",
  
  # Read filtering
  read_min_length = 20,
  read_max_length = Inf,
  read_max_ee = 1,
  read_trunc_length = 150,
  read_trim_left = 0,
  read_trim_right = 0,
  
  # ASV filtering
  asv_min_length = 195,
  asv_max_length = 215,
  genetic_code = "SGC4",
  coding = TRUE,
  
  # Taxonomic filtering
  target_kingdom = "Metazoa",
  target_phylum = "Arthropoda",
  target_class = NA_character_,
  target_order = NA_character_,
  target_family = NA_character_, 
  target_genus = NA_character_, 
  target_species= NA_character_,
  
  # Sample & Taxon filtering
  min_sample_reads = 1000,
  min_taxa_reads= NA_integer_, # Minimum reads per ASV to retain
  min_taxa_ra = 1e-4, # Minimum relative abundance per ASV to retain. 1e-4 is 0.01%
)

write_csv(params, "sample_data/loci_params.csv")
```

## Different or multiple primer sets per sample

```{R}
# Params to add in step_add_parameters
params <- tibble(
  pcr_primers = c("fwhF2-fwhR2nDac", "EIF3LminiF4-EIF3lminiR4"),
  target_gene=c("COI", "EIF3L"),
  phmm = c("reference/phmm/Bactrocera_COI.rds", "reference/phmm/Bactrocera_EIF3L.rds"),
  ref_db = c("reference/COI_idtaxa.rds","reference/EIF3L_idtaxa.rds"),
  blast_db = c("reference/COI_hierarchial.fa.gz", "reference/EIF3L_hierarchial.fa.gz"),
  asv_min_length = c(195, 207),
  asv_max_length = c(215, 227),
  genetic_code = c("SGC4", "SGC0"),
  coding = c(TRUE, TRUE),
  target_kingdom = c("Metazoa", "Metazoa"),
  target_phylum = c("Arthropoda","Arthropoda"),
  target_class = c(NA_character_, NA_character_),
  target_order = c(NA_character_, NA_character_),
  target_family = c(NA_character_, NA_character_), 
  target_genus = c(NA_character_, NA_character_),  
  target_species= c(NA_character_, NA_character_),  
  min_sample_reads = c(1000, 1000),
  min_taxa_reads= c(NA_integer_, NA_integer_ ), # Minimum reads per ASV to retain
  min_taxa_ra = c(1e-4, 1e-4), # Minimum relative abundance per ASV to retain. 1e-4 is 0.01%
)

write_csv(params, "sample_data/loci_params.csv")
```

## Multiple reference databases

```{r}
params <- tibble(
  pcr_primers = c("fwhF2-fwhR2nDac", "EIF3LminiF4-EIF3lminiR4"),
  target_gene=c("COI", "EIF3L"),
  phmm = c("reference/phmm/Bactrocera_COI.rds", "reference/phmm/Bactrocera_EIF3L.rds"),
  ref_db = c("reference/COI_internal_idtaxa.rds;reference/COI_idtaxa.rds","reference/EIF3L_internal_idtaxa.rds;reference/EIF3L_idtaxa.rds"),
  blast_db = c("reference/COI_internal.fa.gz;reference/COI_hierarchial.fa.gz", "reference/EIF3L_internal.fa.gz;reference/EIF3L_hierarchial.fa.gz"),
  asv_min_length = c(195, 207),
  asv_max_length = c(215, 227),
  genetic_code = c("SGC4", "SGC0"),
  coding = c(TRUE, TRUE),
  target_kingdom = c("Metazoa", "Metazoa"),
  target_phylum = c("Arthropoda","Arthropoda"),
  target_class = c(NA_character_, NA_character_),
  target_order = c(NA_character_, NA_character_),
  target_family = c(NA_character_, NA_character_), 
  target_genus = c(NA_character_, NA_character_),  
  target_species= c(NA_character_, NA_character_),  
  min_sample_reads = c(1000, 1000),
  min_taxa_reads= c(NA_integer_, NA_integer_ ), # Minimum reads per ASV to retain
  min_taxa_ra = c(1e-4, 1e-4), # Minimum relative abundance per ASV to retain. 1e-4 is 0.01%
)
write_csv(params, "sample_data/loci_params.csv")
```

# Run pipeline {-}
Now that the sample data sheet and parameters are defined, the pipeline steps can now be visualised, and then run automatically.

## Visualise the pipeline steps

```{r}
# Visualise the planned targets pipeline
tar_glimpse()
```

## Run the pipeline

```{r}
tar_make()
```


# Results

After the run, two directories should have been made within the output folder, one for the unfiltered results, and another for the results after taxonomic subsetting, sample minimum abundance, and ASV minimum abundance filtering. 

## Main results
Within each of these folders there should be a series of outputs:

samdf.csv - This lists the abundance of each ASV and the sample it was detected in
taxtab.csv - This lists the heirarchial taxonomy assigned to each ASV
samdf.csv - This is the sample tracking sheet that lists the sequencing details for each sample
ps.rds - This phyloseq object contains the information from the above 3 files, and can be analysed further using the phyloseq R package, a useful tool for plotting and statistical analysis of metabarcoding datasets: https://joey711.github.io/phyloseq/
asvs.fasta - This contains the amplicon sequence variants infered from the dataset
spp_sum.csv - This is a Species level summary listing the abundance of each Species and the sample it was detected in
raw.csv - This is a large file containing all the above results in a single csv

## Quality control

The pipeline also outputs some quality control plots that should be checked to ensure the pipeline has run correctly and the outputs are as desired

### Sequencing QC
QScore
avg_intensity
PF_clusters
Index switching

### Sample QC
prefilt_qualplots
postfilt_qualplots

### DADA2 QC
Error_model

### Taxonomic assignment
locus_taxonomic_assignment.pdf 
locus_taxonomic_assignment.csv - lis

### Seqtab

### Pipeline summary
read_tracker.csv


