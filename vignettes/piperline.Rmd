---
title: "pipeRline script for Hemiptera metabarcoding analysis"
author: "Alexander Piper"
date: "21/09/2018"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}

# Knitr global setup - change eval to true to run code

library(knitr)
knitr::opts_chunk$set(echo = TRUE, eval=FALSE, fig.show = "hold", fig.keep = "all")
opts_knit$set(root.dir = 'C:/Users/ap0y/Dropbox/Work Projects/PHD/Metabarcoding/Hemiptera_metabarcoding')
opts_chunk$set(dev = 'png')
```
![Piperline](C:/Users/ap0y/Dropbox/Work Projects/PHD/Metabarcoding/Hemiptera_metabarcoding/piperline.png)


# Introduction

This is version 0.3 of the PipeRline metabarcoding analysis workflow, as implemented for the analysis of the iMAPPests data

A typical metabarcoding analysis retains tens to hundreds of millions of reads following quality filtering, and assigning taxonomy to each read individually can be a computationally intensive and lengthy process, taking multiple days on powerful computing clusters. To alleviate this computational burden, sequence dereplication (Traditionally OTU clustering) used to collapse similar sequences into distinct subgroups, each with a single representative sequence and an associated abundance or read count, enabling rapid downstream analysis using the representative rather than the full dataset. 

A challenge of many OTU methods is that memory requirements and running time scale quadratically with sequencing depth, because these methods rely on pairwise comparisons between all sequencing reads. This pipeline is instead based around the DADA2 package (https://benjjneb.github.io/dada2/index.html) to perform the majority of the sequence read processing. DADA2 breaks this quadratic scaling by **processing samples independently**. This is possible because DADA2 infers exact sequence variants, and exact sequences are consistent labels that can be directly compared across separately processed samples. This isn’t the case for OTUs, as the boundaries and membership of de novo OTUs depend on the rest of the dataset and the arbitrary clustering threshold chosen, and thus are only valid and consistent when all sequences are pooled together for OTU picking.

Separable sample processing allows DADA2’s running time to scale linearly in the number of samples, and its memory requirements to remain nearly flat. In addition, the most costly portion of the workflow is fully data parallelizable, and can be spread across non-interacting compute nodes such as the BASC system located at AgriBio.

For biosecurity, decisions are made on species level taxonomy. For metabarcoding, the assignment of Linnaean taxonomy (species, genus etc.) is conducted through an automated classification approach. The main challenges that automated taxonomic classification faces is that all reference databases are by nature incomplete, and may also contain mis-annotated sequences. This can cause two problems, firstly Over-classification, or when query sequences are incorrectly assigned to the wrong species-level taxonomy due to the absence of reference data which can potentially lead to incorrect classification of previously un-sequenced but probably innocuous taxa as a known pest, due to the pest having an existing DNA barcode. And secondly under-classifciation where the algorithm is too conservative and the query is not assigned to taxonomy, resulting in the potential to miss a serious pest

The PipeRline currently uses the RDP naive Bayesian classifier algorithm (as natively implementated in the DADA2 R Package) for assigning taxonomy. This classifier splits the query sequences into 8-mers and uses repeated random sampling of these 8-mers against the reference database to estimate the confidence of the query sequences inclusion into each taxonomic rank. In an ideal case, only a single possible taxonomic assignment will obtain a high level of probability, whereas alternate outcomes will obtain probabilities close to zero. In cases where there may be ambiguity due to imperfect reference data and multiple taxonomic outcomes obtain similar probabilities, the sequence may still be robustly assigned to a higher taxonomic rank (i.e. family), providing important information about sample composition and possible presence of novel taxa without over-classifying and producing false positives. 

In the DADA2 package, the RDP classifier is called upon using the 'assignTaxonomy' function. This function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least minBoot bootstrap confidence.

Importantly, while the RDP classifier will assign taxonomy across multiple ranks (e.g. Kingdom to Genus), it will not assign species level taxonomy to the sequences. DADA2 therefore implements a second method, assignSpecies that uses exact string matching against a reference database to assign Genus species binomials ONLY if it is an unambiguous match.

NOTE: This functionality was designed for 16s marker gene sequencing, where recent results indicate that exact matching (or 100% identity) with exact sequence variants is the only appropriate method for species-level assignment to high-throughput 16S amplicon data, however this is perhaps too conservative for the fast evolving COI gene. Therefore future versions of the PipeRline should explore alternative taxonomic assignment methods for species level taxonomy

## Reference Databases

the DADA2 creators maintain formatted reference databases for both Bacterial and Fungi classification: https://benjjneb.github.io/dada2/training.html however for COI you will need to use a custom database 

For Kingdom to genus classification: assignTaxonomy(...) expects a training fasta file (or compressed fasta file) in which the taxonomy corresponding to each sequence is encoded in the id line in the following fashion (the second sequence is not assigned down to level 6):

```{code}
>Level1;Level2;Level3;Level4;Level5;Level6;
ACCTAGAAAGTCGTAGATCGAAGTTGAAGCATCGCCCGATGATCGTCTGAAGCTGTAGCATGAGTCGATTTTCACATTCAGGGATACCATAGGATAC
>Level1;Level2;Level3;Level4;Level5;
CGCTAGAAAGTCGTAGAAGGCTCGGAGGTTTGAAGCATCGCCCGATGGGATCTCGTTGCTGTAGCATGAGTACGGACATTCAGGGATCATAGGATAC
```

For Species level classification: both assignSpecies(...) and addSpecies(...) expect the training data to be provided in the form of a fasta file (or compressed fasta file), with the id line formatted as follows:
```{code}
> ID Genus species
ACCTAGAAAGTCGTAGATCGAAGTTGAAGCATCGCCCGATGATCGTCTGAAGCTGTAGCATGAGTCGATTTTCACATTCAGGGATACCATAGGATAC
> ID Genus species
CGCTAGAAAGTCGTAGAAGGCTCGGAGGTTTGAAGCATCGCCCGATGGGATCTCGTTGCTGTAGCATGAGTACGGACATTCAGGGATCATAGGATAC
```

Finally, following taxonomic assignment, both the sequence table and taxonomic table generated by DADA2 are passed to the Phyloseq package (https://joey711.github.io/phyloseq/) for further community analysis and visualisation of data. The phyloseq package is a tool to import, store, analyze, and graphically display complex phylogenetic sequencing data that has already been dereplicated into ESV's along with associated sample data, phylogenetic tree, and/or taxonomic assignment of the ESV's. The package leverages many of the tools available in R for ecology and phylogenetic analysis (vegan, ade4, ape, picante), while also using advanced/flexible graphic systems (ggplot2) to easily produce publication-quality graphics of complex phylogenetic data. phyloseq uses a specialized system of S4 classes to store all related phylogenetic sequencing data as single experiment-level object, making it easier to share data and reproduce analyses


NOTE: Currently the functionality to track the amount of reads making it through each stage is not working, job for next version

# Prior to analysis
The PipeRline workflow assumes that your sequencing data meets certain criteria:

1. Samples have been demultiplexed, i.e. split into individual per-sample fastq files.
    + This is normally handled by the Illumina software (MiSeq), or Tracie Webster (HiSeq).
2. For paired-end sequencing data, the forward and reverse fastq files contain reads in matched order.


## Fill out sample metadata files

You can format the metadata file (which is in a compatible format to the QIIME1 mapping files) in a spreadsheet and validate the format using a google spreadsheet plugin as described on the QIIME2 website. The only required column is the sample id column, which should be first. All other columns should correspond to sample metadata. Below, we use the default filename of metadata.txt.

## Fill out parameters file
    

## Directory setup

Large projects can span multiple sequencing runs, and because different runs can have different error profiles, it is recommended to learn the error rates for each run individually and then merge the sequence tables from each run together into a full-study sequence table.

Sequences from these runs must cover the same gene region if you want to simply merge them together, otherwise the sequences aren’t directly comparable. In practice this means that the same primer sets and the same (or no) trimLeft value was used across runs. Single-reads must also be truncated to the same length (this is not necessary for overlapping paired-reads, as  truncLen doesn’t affect the region covered by the merged reads).

In the PipeRline workflow, in order to not violate the above assumptions, these variables are standardised between each run by looping the analysis scripts over each run. The code to do this requires a specific directory structure for the project. Demultiplexed, per-sample, gzipped fastq files for the forward and reverse reads need to be contained in seperate subdirectories for each sequencing run, the subdirectories for each run need to be preceded by "run(n)_", and contained in the data directory. In addition, the string parsing used for sample names expects filenames of the following format: Forward: samplename_S1_L001_R1_001.trimmed.fastq.gz Reverse: samplename_S1_L001_R2_001.trimmed.fastq.gz  (Default for the AgriBio sequencing machines)

## Recommended directory setup
```{code}
Project_folder/
├── data/
│   ├── run1_mock_samples/
│   │     └── samplename_S1_L001_R1_001.trimmed.fastq.gz
│   │     └── samplename_S1_L001_R2_001.trimmed.fastq.gz
│   ├── run2_trap_samples/
│   └── run3_test/
├── scripts/
│   ├── piperline.rmd
│   └── other_scripts.R
├─── sample_info/
│    └── sample_info.csv
├── output/
│   ├── fig/
│   ├── csv/
│   └── rfiles/
└─── doc
    └── manuscript.doc/
```
    
    

# Part 1: Sequence quality control

Run the QC script

```{bash}
bash ...


```
 

The important part of this, if all looks well, is to inform the filtering parameters. If there is only one part of any amplicon bioinformatics workflow on which you spend time considering the parameters, it should be filtering! 

The max expected error function is used as the primary quality filter, and all reads containing N bases were removed

In order to reduce the amount of  reverse reads violating the MaxEE filter, the reverse reads were truncated at 230bp to remove the quality crash that is typical of illumina sequencers

The above parameters work well for good quality 250nt Hiseq data, but they are not set in stone, and should be changed if they don’t work for your data. If too few reads are passing the filter, increase maxEE and/or reduce truncQ. If quality drops sharply at the end of your reads, reduce truncLen. If your reads are high quality and you want to reduce computation time in the sample inference step, reduce  maxEE

In gray-scale is a heat map of the frequency of each quality score at each base position. The median quality score at each position is shown by the green line, and the quartiles of the quality score distribution by the orange lines. The red line shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same lenghth, hence the flat red line).

The forward reads are generally of good quality. Normally it is advised to trim the last few nucleotides to avoid less well-controlled errors that can arise there. The reverse reads are of slightly worse worse quality, especially at the end, which is common in Illumina sequencing. This isn’t too worrisome, as DADA2 incorporates quality information into its error model which makes the algorithm robust to lower quality sequence, but trimming as the average qualities crash will improve the algorithm’s sensitivity to rare sequence variants. Informed by these profiles, reads can be truncated using truncLen=c(forwardlength, reverselength)

# Fill in parameter sheet

Have examples of choosing good parameters for different types of data


# Run analysis

This step contains a number of steps:

## Trim primers


## Filter and trim


## Infer sequence variants

Every amplicon dataset has a different set of error rates and the DADA2 algorithm makes use of a parametric error model (err) to model this and infer real biological sequence variation from error. The learnErrors method learns this error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

Following error model learning, the core DADA2 algorthim dereplicates all identical sequencing reads are dereplicated into into “unique sequences” with a corresponding “abundance” equal to the number of reads with that unique sequence. The forward and reverse reads are then merged together by aligning the denoised forward reads with the reverse-complement of the corresponding reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least **12 bases**, and are identical to each other in the overlap region. 

Following this step, a sequence variant table is constructed - a higher-resolution version of the OTU table produced by traditional methods.

In order to handle bigger datasets, in this pipeline the samples are read in and processed within a for-loop, so only one sample is fully loaded into memory at a time. This keeps memory requirements quite low, and multiple MiSeq runs, or a Hiseq lane can be processed on 8GB of memory (although more is nice!).

NOTE: Learning error rates is computationally intensive, as it requires multiple iterations of the core algorithm. Therefore if you are analysing a large dataset you can get the algorithm to only learn from a subset of the reads using (nbases = 1e8, randomize=TRUE) As a rule of thumb, a million 100nt reads (or 100M total bases) is more than adequate to learn the error rates.

While It can be worthwhile to plot the fit of error modelling before dereplication, this is not useful for abig-data version of sample inference. The error rates for each possible transition (A→C, A→G, …) are shown. Points are the observed error rates for each consensus quality score. The black line shows the estimated error rates after convergence of the machine-learning algorithm. The red line shows the error rates expected under the nominal definition of the Q-score. You should expect to see the estimated error rates (black line) fit well to the observed rates (points), and the error rates dropping with increased quality as expected. If this plot seems wildly off, it may be worth running the previous steps again using the full dataset to learn errors if you are not already doing so, or make sure primers are correctly trimmed.
 
## Merge Runs, Remove Chimeras

Now that all of the individual run sequence tables are created, they are then merged into a full-study sequence table. The core dada method corrects substitution and indel errors, but chimeras remain. The frequency of chimeric sequences varies substantially from dataset to dataset, and depends on on factors including experimental procedures and sample complexity. Fortunately, the accuracy of the sequence variants after denoising makes identifying chimeras simpler than it is when dealing with fuzzy OTUs.  Chimeric sequences are identified and removed using removeBimeraDenovo if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.The "consensus" chimera removal method works better on large studies, but the "pooled" method is also an option. Following chimera removal, any identical variants with the only difference being length variation are collapsed using collapseNoMismatch.

## Assign taxonomy

At this point we will assign kingdom to genus taxonomy to the sequence reads using the RDP classifier, and then add species level taxonomy to this using exact matching

The parameters this takes in depends on the taxonomic classifier desired. The currently supported classifiers are:

# Create phylogenetic tree


# Analysis of outputs


## Create phyloseq and filter
This can either be conductd using seqscope interactively, or done in R

Following taxonomic assignment, both the sequence table and taxonomic table are passed to the Phyloseq R package for further community analysis and visualisation of data

This involves loading in the sample info csv to merge with the sequence and taxonomy trables

```{r create PS, eval = TRUE}
seqtab.nochim <- readRDS("data/seqtab_final.rds") # CHANGE ME to where you want sequence table saved
tax_plus <- readRDS("data/tax_boot80_final.rds") # CHANGE ME ...

#Load sample information
## ---- samdat ----
samdf <- read.csv("sample_data/Sample_info.csv", header=TRUE) ###Change this
#samdf$SampleID <- paste0(samdf$sample_name)
samdf <- samdf[!duplicated(samdf$SampleID),] #Remove duplicate entries for reverse reads

rownames(samdf) <- samdf$SampleID
keep.cols <- c("collection_date", "biome", "target_gene", "feature",
"sample_name" ,"SampleID","experimental_factor")
samdf <- samdf[rownames(seqtab.nochim), keep.cols]

## ---- phyloseq ----
ps <- phyloseq(tax_table(tax_plus), sample_data(samdf),
               otu_table(seqtab.nochim, taxa_are_rows = FALSE))
##save phyloseq object
saveRDS(ps, "data/ps_genus_spp_80.rds") 

#Display samDF
head(samdf)

##Define transformation functions

#Proportion function
proportions = function(x){
  xprop = (x / sum(x))
  return(xprop)
}
##Transform data to proportions and set low proportions to zero
filterfun = function(x){
  xprop = (x / sum(x)) #Convert to proportions
  xprop[xprop < (1e-4)] <- 0 ## remove taxa under 0.0001, which is 0.01%
  return(xprop)
}

#Subset data to Athropoda only 
ps1 = subset_taxa(ps, Phylum == "Arthropoda") 

#Export raw data
rawexport <- psmelt(ps1)
write.csv(rawexport, file = "rawarthropod.csv")

#Export proportion data
ps2 <- transform_sample_counts(ps1, fun = proportions)

sum_gen <- summarize_taxa(ps2, "Genus", "SampleID")
sum_gen <- spread(sum_gen, key="SampleID", value="totalRA")
write.csv(sum_gen, file = "allgenes_gen_totalsummary.csv")

sum_sp <- summarize_taxa(ps2, "Species", "SampleID")
sum_sp <- spread(sum_sp, key="SampleID", value="totalRA")
write.csv(sum_sp, file = "allgenes_sp_totalsummary.csv")

#Convert data to proportions and apply filter threshold
psFR <- transform_sample_counts(ps1, fun = filterfun)
psFR <- transform_sample_counts(ps1, fun = proportions)

sum_filt <- summarize_taxa(psFR, "Species", "SampleID")
sum_filt <- spread(sum_filt, key="SampleID", value="totalRA")
write.csv(sum_filt, file = "allgenes_sp_filtsummary.csv")

```


